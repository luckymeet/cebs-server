<?xml version="1.0" encoding="UTF-8" ?>
<Configuration>
	<Properties>
		<Property name="springAppName">cebs</Property>
		<!-- 格式化输出：%date表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度 %msg：日志消息，%n是换行符-->
        <!-- %logger{36} 表示 Logger 名字最长36个字符 -->
        <property name="LOG_PATTERN" value="[%d{yyyy-MM-dd HH:mm:ss.SSS}] %-4level [${springAppName},%X{X-B3-TraceId},%X{X-B3-SpanId},%X{X-Span-Export}] ${sys:PID} --- [%t] %logger{36} - %msg%n" />
        <Property name="PATTERN">{"host": "%host", "logger": "%logger", "level": "%level", "message": "[%thread] [%c:%L] --- %message"}%n</Property>
        <!-- 定义日志存储的路径，不要配置相对路径 -->
        <property name="FILE_PATH" value="E:/tem/log" />
        <property name="FILE_NAME" value="log-test" />
	</Properties>

	<Appenders>
		<Console name="Console" target="SYSTEM_OUT">
			<PatternLayout
				pattern= "${LOG_PATTERN}"/>
		</Console>

		<!--文件会打印出所有信息，这个log每次运行程序会自动清空，由append属性决定，适合临时测试用-->
        <File name="Filelog" fileName="${FILE_PATH}/test.log" append="false">
            <PatternLayout pattern="${LOG_PATTERN}"/>
        </File>

        <!-- 这个会打印出所有的info及以下级别的信息，每次大小超过size，则这size大小的日志会自动存入按年份-月份建立的文件夹下面并进行压缩，作为存档-->
        <RollingFile name="RollingFileInfo" fileName="${FILE_PATH}/info.log" filePattern="${FILE_PATH}/${FILE_NAME}-INFO-%d{yyyy-MM-dd}_%i.log.gz">
            <!--控制台只输出level及以上级别的信息（onMatch），其他的直接拒绝（onMismatch）-->
            <ThresholdFilter level="info" onMatch="ACCEPT" onMismatch="DENY"/>
            <PatternLayout pattern="${LOG_PATTERN}"/>
            <Policies>
                <!--interval属性用来指定多久滚动一次，默认是1 hour-->
                <TimeBasedTriggeringPolicy interval="1"/>
                <SizeBasedTriggeringPolicy size="10MB"/>
            </Policies>
            <!-- DefaultRolloverStrategy属性如不设置，则默认为最多同一文件夹下7个文件开始覆盖-->
            <DefaultRolloverStrategy max="15"/>
        </RollingFile>
        <Kafka name="Kafka" topic="log-test">
            <PatternLayout pattern="${LOG_PATTERN}" />
            <Property name="bootstrap.servers">39.105.58.23:9092</Property>
        </Kafka>
	</Appenders>

	<Loggers>
		<Root level="info">
			<AppenderRef ref="Console" />
			<AppenderRef ref="Kafka" />
			<AppenderRef ref="Filelog" />
		</Root>
		<logger name="druid.sql.Statement" level="debug"
			additivity="true">
			<appender-ref ref="Console" />
		</logger>
		<logger name="druid.sql.ResultSet" level="debug"
			additivity="true">
			<appender-ref ref="Console" />
		</logger>
	</Loggers>

	<bean id="log-filter" class="com.alibaba.druid.filter.logging.Slf4jLogFilter">
		<property name="connectionLogEnabled" value="false" />
		<property name="statementLogEnabled" value="false" />
		<property name="resultSetLogEnabled" value="true" />
		<property name="statementExecutableSqlLogEnable" value="true" />
	</bean>
</Configuration>
